package ru.practicum.analytics;

import com.fasterxml.jackson.databind.ObjectMapper;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.functions;
import org.apache.spark.sql.streaming.StreamingQuery;
import org.apache.spark.sql.streaming.Trigger;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructType;
import org.apache.spark.sql.types.StructField;

import java.util.Properties;
import java.util.concurrent.TimeUnit;

public class ProductAnalyticsApp {
    private static final ObjectMapper mapper = new ObjectMapper();

    public static void main(String[] args) throws Exception {
        // –°–æ–∑–¥–∞–µ–º Spark —Å–µ—Å—Å–∏—é
        SparkSession spark = SparkSession.builder()
                .appName("ProductAnalytics")
                .config("spark.sql.adaptive.enabled", "true")
                .config("spark.sql.adaptive.coalescePartitions.enabled", "true")
                .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
                .master("local[*]") // –í production —É–∫–∞–∂–∏—Ç–µ –≤–∞—à Spark master
                .getOrCreate();

        spark.sparkContext().setLogLevel("WARN");

        System.out.println("üöÄ Starting Product Analytics Application...");

        // 1. –ß—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Kafka
        Dataset<Row> kafkaData = spark
                .readStream()
                .format("kafka")
                .option("kafka.bootstrap.servers", "kafka-0:1092,kafka-1:2092")
                .option("subscribe", "products")
                .option("startingOffsets", "latest")
                .option("kafka.security.protocol", "SASL_SSL")
                .option("kafka.sasl.mechanism", "PLAIN")
                .option("kafka.sasl.jaas.config",
                        "org.apache.kafka.common.security.plain.PlainLoginModule required " +
                                "username=\"admin\" password=\"admin\";")
                .option("kafka.ssl.truststore.location", "/etc/kafka/secrets/kafka.truststore.jks")
                .option("kafka.ssl.truststore.password", "password")
                .option("kafka.ssl.keystore.location", "/etc/kafka/secrets/kafka.keystore.pkcs12")
                .option("kafka.ssl.keystore.password", "password")
                .load();

        // –°—Ö–µ–º–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –¥–∞–Ω–Ω—ã—Ö
        StructType productSchema = new StructType(new StructField[]{
                new StructField("product_id", DataTypes.StringType, true),
                new StructField("name", DataTypes.StringType, true),
                new StructField("category", DataTypes.StringType, true),
                new StructField("price", DataTypes.DoubleType, true),
                new StructField("rating", DataTypes.DoubleType, true),
                new StructField("timestamp", DataTypes.TimestampType, true)
        });

        // 2. –ü–∞—Ä—Å–∏–Ω–≥ JSON –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        Dataset<Row> products = kafkaData
                .selectExpr("CAST(value AS STRING) as json_value")
                .filter(functions.col("json_value").isNotNull())
                .select(functions.from_json(functions.col("json_value"), productSchema).as("data"))
                .select("data.*")
                .filter(functions.col("product_id").isNotNull())
                .withColumn("processing_time", functions.current_timestamp());

        // 3. –ê–ù–ê–õ–ò–¢–ò–ö–ê: –†–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏

        // –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ 1: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        Dataset<Row> categoryStats = products
                .groupBy("category")
                .agg(
                        functions.count("product_id").as("product_count"),
                        functions.avg("price").as("avg_price"),
                        functions.avg("rating").as("avg_rating"),
                        functions.max("price").as("max_price"),
                        functions.min("price").as("min_price")
                )
                .withColumn("analysis_type", functions.lit("category_statistics"))
                .withColumn("timestamp", functions.current_timestamp());

        // –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ 2: –¢–æ–ø –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –ø–æ —Ä–µ–π—Ç–∏–Ω–≥—É
        Dataset<Row> topRatedProducts = products
                .filter(functions.col("rating").isNotNull())
                .orderBy(functions.col("rating").desc())
                .limit(10)
                .select(
                        functions.col("product_id"),
                        functions.col("name"),
                        functions.col("category"),
                        functions.col("rating"),
                        functions.col("price")
                )
                .withColumn("analysis_type", functions.lit("top_rated_products"))
                .withColumn("timestamp", functions.current_timestamp());

        // –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ 3: –¶–µ–Ω–æ–≤–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        Dataset<Row> priceDistribution = products
                .withColumn("price_range",
                        functions.when(functions.col("price").lt(100), "Budget")
                                .when(functions.col("price").between(100, 500), "Medium")
                                .when(functions.col("price").between(500, 1000), "Expensive")
                                .otherwise("Premium"))
                .groupBy("price_range")
                .agg(
                        functions.count("product_id").as("product_count"),
                        functions.avg("rating").as("avg_rating")
                )
                .withColumn("analysis_type", functions.lit("price_distribution"))
                .withColumn("timestamp", functions.current_timestamp());

        // –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ 4: –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        Dataset<Row> categoryRecommendations = products
                .groupBy("category")
                .agg(
                        functions.count("product_id").as("product_count"),
                        functions.avg("rating").as("avg_rating"),
                        functions.avg("price").as("avg_price")
                )
                .withColumn("recommendation_score",
                        functions.col("product_count") * functions.col("avg_rating"))
                .orderBy(functions.col("recommendation_score").desc())
                .withColumn("recommendation_type", functions.lit("popular_category"))
                .withColumn("timestamp", functions.current_timestamp());

        // 4. –ó–ê–ü–ò–°–¨ –í HDFS

        // –ó–∞–ø–∏—Å—å —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ HDFS
        StreamingQuery rawDataQuery = products
                .writeStream()
                .outputMode("append")
                .format("parquet")
                .option("path", "hdfs://namenode:9000/data/products/raw")
                .option("checkpointLocation", "hdfs://namenode:9000/checkpoints/products_raw")
                .trigger(Trigger.ProcessingTime("1 minute"))
                .start();

        // –ó–∞–ø–∏—Å—å –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –≤ HDFS
        StreamingQuery analyticsQuery = categoryStats
                .writeStream()
                .outputMode("complete")
                .format("parquet")
                .option("path", "hdfs://namenode:9000/data/analytics/category_stats")
                .option("checkpointLocation", "hdfs://namenode:9000/checkpoints/category_stats")
                .trigger(Trigger.ProcessingTime("2 minutes"))
                .start();

        // 5. –ó–ê–ü–ò–°–¨ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ô –í KAFKA

        // –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è Kafka
        Dataset<Row> kafkaRecommendations = categoryRecommendations
                .select(
                        functions.struct(
                                functions.col("category"),
                                functions.col("recommendation_score"),
                                functions.col("avg_rating"),
                                functions.col("avg_price"),
                                functions.col("recommendation_type"),
                                functions.col("timestamp")
                        ).as("value")
                )
                .select(functions.to_json(functions.col("value")).as("value"));

        // –ó–∞–ø–∏—Å—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –≤ Kafka
        StreamingQuery kafkaQuery = kafkaRecommendations
                .writeStream()
                .outputMode("update")
                .format("kafka")
                .option("kafka.bootstrap.servers", "kafka-0:1092,kafka-1:2092")
                .option("topic", "product-recommendations")
                .option("checkpointLocation", "hdfs://namenode:9000/checkpoints/kafka_recommendations")
                .option("kafka.security.protocol", "SASL_SSL")
                .option("kafka.sasl.mechanism", "PLAIN")
                .option("kafka.sasl.jaas.config",
                        "org.apache.kafka.common.security.plain.PlainLoginModule required " +
                                "username=\"admin\" password=\"admin\";")
                .trigger(Trigger.ProcessingTime("3 minutes"))
                .start();

        System.out.println("‚úÖ All streaming queries started successfully");

        // –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        spark.streams().awaitAnyTermination();
    }
}